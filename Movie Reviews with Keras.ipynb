{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter('ignore')\nsns.set(rc={'figure.figsize' : (12, 6)})\nsns.set_style(\"darkgrid\", {'axes.grid' : True})\nimport skimage\n\n# Импортируем TensorFlow и tf.keras\nimport tensorflow as tf\nfrom tensorflow import keras","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/IMDB Dataset.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explore the data\nLet's take a moment to understand the format of the data. The dataset comes unprocessed: each example is an array of words representing the movie review. Each label is a string value of either negative or positive sentiment of author of review."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of poitive and negative reviews\ndata.sentiment.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets encode labels: each label is an integer value of either 0 or 1, where 0 is a negative review, and 1 is a positive review.\nfrom sklearn import preprocessing\nlabel_encoder = preprocessing.LabelEncoder()\ndata['sentiment'] = label_encoder.fit_transform(data['sentiment'])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now, let's see the average number of words per sample\nplt.figure(figsize=(10, 6))\nplt.hist([len(sample) for sample in list(data['review'])], 50)\nplt.xlabel('Length of samples')\nplt.ylabel('Number of samples')\nplt.title('Sample length distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now use the **CountVectorizer** provided by the scikit-learn library to vectorize sentences. It takes the words of each sentence and creates a vocabulary of all the unique words in the sentences. This vocabulary can then be used to create a feature vector of the count of the words:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\n# So, we get such structure:\n#        | word1  | word2  |  word3 | word4\n# text1  |   1    |    1   |   1    |   0\n# text2  |   0    |    1   |   1    |   0\n# text3  |   2    |    1   |   0    |   0\n# text4  |   0    |    0   |   0    |   1\nvect_texts = vectorizer.fit_transform(list(data['review']))\n# ['word1', 'word2', 'word3', 'word4']\nall_ngrams = vectorizer.get_feature_names()\nnum_ngrams = min(50, len(all_ngrams))\nall_counts = vect_texts.sum(axis=0).tolist()[0]\n\nall_ngrams, all_counts = zip(*[(n, c) for c, n in sorted(zip(all_counts, all_ngrams), reverse=True)])\nngrams = all_ngrams[:num_ngrams]\ncounts = all_counts[:num_ngrams]\n\nidx = np.arange(num_ngrams)\n\n# Let's now plot a frequency distribution plot of the most seen words in the corpus.\nplt.figure(figsize=(30, 30))\nplt.bar(idx, counts, width=0.8)\nplt.xlabel('N-grams')\nplt.ylabel('Frequencies')\nplt.title('Frequency distribution of ngrams')\nplt.xticks(idx, ngrams, rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, the highest frequency words are the stop words. We not consider them while performing our analysis, as they don't provide insights as to what the sentiment of the document might be or to which class a document might belong."},{"metadata":{},"cell_type":"markdown","source":"Let's now prepare the data to feed into the model. For the data preparation step we will get bigrams and unigrams from the data and encode it using tf-idf. And will select the top 20000 features from the vector of tokens. Discard features that occurs less than two times, and will f_classif to get feature importance."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nNGRAM_RANGE = (1, 2)\nTOP_K = 20000\nTOKEN_MODE = 'word'\nMIN_DOC_FREQ = 2\n\ndef ngram_vectorize(texts, labels):\n    kwargs = {\n        'ngram_range' : NGRAM_RANGE,\n        'dtype' : 'int32',\n        'strip_accents' : 'unicode',\n        'decode_error' : 'replace',\n        'analyzer' : TOKEN_MODE,\n        'min_df' : MIN_DOC_FREQ,\n    }\n    # Learn Vocab from train texts and vectorize train and val sets\n    tfidf_vectorizer = TfidfVectorizer(**kwargs)\n    transformed_texts = tfidf_vectorizer.fit_transform(texts)\n    \n    # Select best k features, with feature importance measured by f_classif\n    # Set k as 20000 or (if number of ngrams is less) number of ngrams   \n    selector = SelectKBest(f_classif, k=min(TOP_K, transformed_texts.shape[1]))\n    selector.fit(transformed_texts, labels)\n    transformed_texts = selector.transform(transformed_texts).astype('float32')\n    return transformed_texts\n# Vectorize the data\nvect_data = ngram_vectorize(data['review'], data['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer()\ntr_texts = tfidf.fit_transform(data['review'])\ntr_texts.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split data to target (y) and features (X)\nX = vect_data.toarray()\ny = (np.array(data['sentiment']))\n\n# Here we split data to training and testing parts\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\nprint(\"Train dataset shape: {0}, \\nTest dataset shape: {1}\".format(X_train.shape, X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build the model\nThe neural network is created by stacking layers—this requires two main architectural decisions:\n\n1. *How many layers to use in the model?*\n2. *How many hidden units to use for each layer?*\n\nIn this example, the input data consists of an array of word-probabilities. The labels to predict are either 0 or 1. Let's build a model for this problem:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.keras import models\nfrom tensorflow.python.keras.layers import Dense\nfrom tensorflow.python.keras.layers import Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, let's create a function that returns the appropriate number of units and the activation for the last layer.\ndef get_last_layer_units_and_activation(num_classes):\n    if num_classes == 2:\n        activation = 'sigmoid'\n        units = 1\n    else:\n        activation = 'softmax'\n        units = num_classes\n    return units, activation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# input shape is the vocabulary count used for the movie reviews (10,000 words)\nDROPOUT_RATE = 0.2\nUNITS = 64\nNUM_CLASSES = 2\nLAYERS = 2\ninput_shape = X_train.shape[1:]\n\nop_units, op_activation = get_last_layer_units_and_activation(NUM_CLASSES)\n\nmodel = keras.Sequential()\n# Applies Dropout to the input\nmodel.add(Dropout(rate=DROPOUT_RATE, input_shape=input_shape))\nfor _ in range(LAYERS-1):\n    model.add(Dense(units=UNITS, activation='relu'))\n    model.add(Dropout(rate=DROPOUT_RATE))\n    \nmodel.add(Dense(units=op_units, activation=op_activation))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Compile the model\nBefore the model is ready for training, it needs a few more settings. These are added during the model's compile step:\n\n* **Loss function** —This measures how accurate the model is during training. We want to minimize this function to \"steer\" the model in the right direction.\n* **Optimizer** —This is how the model is updated based on the data it sees and its loss function.\n* **Metrics** —Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified."},{"metadata":{"trusted":true},"cell_type":"code","source":"LEARNING_RATE = 1e-3\n\n# Compile model with parameters\nif NUM_CLASSES == 2:\n    loss = 'binary_crossentropy'\nelse:\n    loss = 'sparse_categorical_crossentropy'\noptimizer = tf.keras.optimizers.Adam(lr=LEARNING_RATE)\nmodel.compile(optimizer=optimizer, loss=loss, metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 100\nBATCH_SIZE = 128\n\n# Create callback for early stopping on validation loss. If the loss does\n# not decrease on two consecutive tries, stop training\ncallbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]\n\n# Train and validate model\n# To start training, call the model.fit method—the model is \"fit\" to the training data.\n# Note that fit() will return a History object which we can use to plot training vs. validation accuracy and loss.\nhistory = model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_test, y_test), verbose=1, batch_size=BATCH_SIZE, callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next, compare how the model performs on the test dataset:\ntest_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\nprint('Test loss:', test_loss)\nprint('Test accuracy:', test_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot training and validation accuracy as well as loss.\ndef plot_history(history):\n    accuracy = history.history['acc']\n    val_accuracy = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n    epochs = range(1,len(accuracy) + 1)\n    \n    # Plot accuracy  \n    plt.figure(1)\n    plt.plot(epochs, accuracy, 'b', label='Training accuracy')\n    plt.plot(epochs, val_accuracy, 'g', label='Validation accuracy')\n    plt.title('Training and validation accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    # Plot loss\n    plt.figure(2)\n    plt.plot(epochs, loss, 'b', label='Training loss')\n    plt.plot(epochs, val_loss, 'g', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Save the model\nThen we can use model.save(filepath) to save a Keras model into a single HDF5 file which will contain:\n* the architecture of the model, allowing to re-create the model\n* the weights of the model\n* the training configuration (loss, optimizer)\n* the state of the optimizer, allowing to resume training exactly where you left off."},{"metadata":{"trusted":true},"cell_type":"code","source":" # Save model\nmodel.save('IMDB_model_dropout_nn.h5')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}